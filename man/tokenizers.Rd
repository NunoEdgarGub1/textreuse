% Generated by roxygen2 (4.1.1): do not edit by hand
% Please edit documentation in R/tokenizers.R
\name{tokenizers}
\alias{tokenize_ngrams}
\alias{tokenize_sentences}
\alias{tokenize_words}
\alias{tokenizers}
\title{Split texts into tokens}
\usage{
tokenize_words(string, lowercase = TRUE)

tokenize_sentences(string, lowercase = TRUE)

tokenize_ngrams(string, lowercase = TRUE, n = 3)
}
\arguments{
\item{string}{A character vector of length 1 to be tokenized.}

\item{lowercase}{Should the tokens be made lower case?}

\item{n}{For n-gram tokenizers, the number of words in each n-gram.}
}
\value{
A character vector containing the tokens.
}
\description{
These functions each turn a text into tokens. The \code{tokenize_ngrams}
functions returns shingled n-grams.
}
\details{
These functions will strip all punctuation.
}

